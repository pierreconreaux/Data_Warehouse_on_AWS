# Data Warehouse on AWS

## Project Description

In this project, we'll apply our knowledge on ***data warehouses*** and ***AWS*** to build an ***ETL pipeline*** for a database hosted on ***Redshift***. To complete the project, we'll need to load data from S3 to staging tables on Redshift and execute SQL statements that create the analytics tables from these staging tables.

## What we'll need to do

1. Understand the datasets and design schemas for your fact and dimension tables
2. Create an AWS cluster and an IAM role
3. Write SQL query to create tables 
4. Write SQL query to etl
5. Write SQL query to test our data warehouse

## Libraries

```
import pandas as pd #Python Data Analysis Library
import boto3 #Boto is the Amazon Web Services (AWS) SDK for Python
import json #The json library can parse JSON from strings or files
import configparser #This module provides the ConfigParser class
import psycopg2 #PostgreSQL database adapter for the Python programming language 
```

## Projet Datasets

We'll be working with two datasets that reside in S3. Here are the S3 links for each:

* Song data: ```s3://udacity-dend/song_data```
* Log data: ```s3://udacity-dend/log_data```

Log data json path: ```s3://udacity-dend/log_json_path.json```

## Song Dataset

The first dataset is a subset of real data from the Million Song Dataset [Million Song Dataset](http://millionsongdataset.com/). Each file is in ***JSON format*** and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.

```
song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json
````

And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.

```
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```

## Log Dataset

The second dataset consists of log files in ***JSON format*** generated by this event simulator based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.

The log files in the dataset we'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.

```
log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json
```

And below is an example of what the data in a log file, 2018-11-12-events.json, looks like.

![alt text](/Picture/log-data.png)

## Schema for Song Play Analysis

Using the song and event datasets, we'll need to create a star schema optimized for queries on song play analysis. This includes the following tables.

### Fact Table

1. ***songplays*** - records in event data associated with song plays i.e. records with page ```NextSong```
* songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent

### Dimension Tables

2. ***users*** - users in the app
* user_id, first_name, last_name, gender, level

3. ***songs*** - songs in music database
* song_id, title, artist_id, year, duration

4. ***artists*** - artists in music database
* artist_id, name, location, lattitude, longitude

5. ***time*** - timestamps of records in songplays broken down into specific units
* start_time, hour, day, week, month, year, weekday

![alt text](/Picture/star_schema.png)

## Project Template

The initial project template includes five files:

* ***create_table.py*** is where we'll create our fact and dimension tables for the star schema in Redshift.
* ***etl.py*** is where we'll load data from S3 into staging tables on Redshift and then process that data into your analytics tables on Redshift.
* ***sql_queries.py*** is where we'll define you SQL statements, which will be imported into the two other files above. 
* ***README.md*** is where we'll provide discussion on our process and decisions for this ETL pipeline (you are reading it now).
* ***dwh.cfg*** is where we'll provide necessary information for AWS (information will be use in other files)

I've add three files:

* ***AWS_cluster_and_role.ipynb*** is where we'll create our cluser and role on aws. If you want you can create it directly on AWS (https://docs.aws.amazon.com/eks/latest/userguide/service_IAM_role.html)
* ***aws.cfg*** is where we'll provide information to create cluster and aws role. I've created this second cfg gile to not modify the first one, but with small code change you can use just one cfg file.
* ***Test.ipynb*** is where we'll test our data warehouse with some query using magic function.

## Project Steps

1. Create AWS cluster and role with ***AWS_cluster_and_role.ipynb*** and ***aws.cfg***.
2. Write SQL query in sql_queries.py. In the document I've added some information on particular choices, as on data type or sort key.
3. Run create_tables.py on terminal
```
python create_tables.py
```
4. Check table create on Test.ipynb. Take care to not launch all queries in Test.ipynb but just necessaries one.
5. Run etl.py on terminal
```
python etl.py
```
6. Check table in Test.ipynb

When you finish to work, don't forget to deleting your cluster. You can find some useful line of code in ***AWS_cluster_and_role.ipynb*** to help you.

## Sources:

Data Engineering Nanodegree: part 3 Cloud Data Warehouses 

https://www.udacity.com/course/data-engineer-nanodegree--nd027

Regarding Sortkey and Distkey

https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-sort-key.html

Distribution styles

https://docs.aws.amazon.com/redshift/latest/dg/c_choosing_dist_sort.html

AWS distribution exemple 

https://docs.aws.amazon.com/redshift/latest/dg/c_Distribution_examples.html

Create table on AWS

https://docs.aws.amazon.com/redshift/latest/dg/r_CREATE_TABLE_NEW.html

Date from BigInt

https://stackoverflow.com/questions/25371543/convert-bigint-to-date-in-postgresql?rq=1

Markdown quick reference

https://wordpress.com/support/markdown-quick-reference/
